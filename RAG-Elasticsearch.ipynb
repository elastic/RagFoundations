{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71892a06-c8b6-4e12-a5de-8a6ff6b30b40",
   "metadata": {},
   "source": [
    "# Interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583615eb-5d10-413f-80f2-a65cf808c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6cc243-4c2b-4317-bc2b-8daea3ebd29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0574df5e-cfca-452f-9621-ef0711855ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env.instruqt')\n",
    "openai_api_key =  os.environ.get(\"LLM_APIKEY\") \n",
    "url = os.environ.get(\"LLM_PROXY_URL\") \n",
    "openai_api_base = f\"https://{url}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b42c2-44ff-4f06-8ec8-f49591f71f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"OPENAI_BASE_URL\"] = openai_api_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa78bd-06f7-45e5-b5cc-449273e5a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1a5e9-bbe3-4a2d-b2bf-a2524c3c8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_host = os.getenv(\"ELASTICSEARCH_URL\", None)\n",
    "es_api_key = os.getenv(\"ELASTICSEARCH_APIKEY\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec541826-4654-400f-9647-b7e2cdd6c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "     hosts=[f\"{es_host}\"],\n",
    "     api_key=es_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d5fd3-e9c1-44dd-a4fe-b9ad55f0b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM is from OpenAI \n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aa7ee8-3fa6-4123-a4ba-8c8b6d14d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start with a simple, one-pass interacation with the LLM. The function call2llm takes a systems_prompt, which is the \n",
    "#persona the system assumes in the interaction, and \"users_prompt\" which is the input from the user chatting with the LLM\n",
    "\n",
    "def call2llm(systems_prompt, users_prompt):\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": systems_prompt},\n",
    "            {\"role\": \"user\", \"content\": users_prompt}\n",
    "        ],\n",
    "        model=\"gpt-4.1\",\n",
    "        temperature=0.000001  # low means consistent LLM responses (high means more creative)\n",
    "    )\n",
    "    response = response.choices[0].message\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c11ff-ea9f-415d-ab26-bf92b431ce5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test\n",
    "llm_answer = call2llm(\"You're a helpful assistant\", \"What is 2+2?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b3aed-1ba0-4281-8e0e-9b7404d3a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer2 = call2llm(\"You're a helpful assistant\", \"What did we just sum?\")\n",
    "print(llm_answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf9022-2b5c-407a-8af4-2c00bf9a5253",
   "metadata": {},
   "source": [
    "No memory in call2llm of what happened previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396d44b-1eea-48ad-b9b6-580becf231f1",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde639e-30c4-48f6-a4ee-6a5a1dc5aa26",
   "metadata": {},
   "source": [
    "#### Implement instead as a python class, which will help in adding conversational memory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97d721-4d35-4536-84ce-8cd9fd96bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatWithLlm:\n",
    "    def __init__(self,systems_prompt=\"assistant\",model=\"gpt-4.1\"):\n",
    "        self.systems_prompt = systems_prompt\n",
    "        self.model = model\n",
    "        self.history = [{\"role\":\"system\",  \n",
    "                         \"content\":systems_prompt}]          #history helps us \"keep memory\" of what happened before\n",
    "   \n",
    "    def call2llm(self, users_prompt, temperature=0.00001):   #low temperature means consistent LLM responses (high means more creative)\n",
    "        client = OpenAI(api_key=openai_api_key)\n",
    "        self.history.append({\"role\": \"user\", \"content\": users_prompt})   #user role prompts the LLM \n",
    "        response = client.chat.completions.create(\n",
    "            messages=self.history,\n",
    "            model=self.model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        response_llm = str(response.choices[0].message.content)\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response_llm})\n",
    "        return response_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f5910-fb8c-48d0-b0ad-ed0ed456ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with an instance of the ChatWithLlm class\n",
    "chat = ChatWithLlm(\"You're a helpful assistant\")\n",
    "llm_answer =  chat.call2llm(\"What is 2 + 2?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41966b48-e016-44af-ac48-04a137d9dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer =  chat.call2llm(\"What did I just ask you?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93b8bb-ca4a-4473-b060-2ce3aa5157a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer =  chat.call2llm(\"How did you remember what was asked?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3b313-b2eb-4cf9-92f9-9316e6a07512",
   "metadata": {},
   "source": [
    " <br>\n",
    " <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7a4b7-ff7c-41ed-a8fe-91590c88cc1d",
   "metadata": {},
   "source": [
    "## RAG solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94b5d7-98d8-471e-bfec-93776b027c82",
   "metadata": {},
   "source": [
    "Finally here is the python class that performs our RAG solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521b8f4-cc80-44c3-80c7-2636557549c6",
   "metadata": {},
   "source": [
    "Elastic_rag both queries Elastisearch and feeds those docs to the LLM in a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29612011-c1e7-4e4c-a986-3542daf0209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elasticsearch_rag:\n",
    "    def __init__(self, systems_prompt=\"You are a helpful assistant.\", model=\"gpt-4.1\"):                \n",
    "        #self.previous_response_id = None\n",
    "        self.systems_prompt = systems_prompt\n",
    "        self.model = model \n",
    "        self.history = [{\"role\": \"system\", \"content\": systems_prompt}]\n",
    "\n",
    "    #retrieve documents from Elasticsearch\n",
    "    def retrieve(self, query,  top_n=2, search_template=\"RAG_application\"):\n",
    "        params = {\"query_string\": query}\n",
    "        params[\"size\"]=top_n\n",
    "        response = es.search_application.search(name=search_template, params=params)\n",
    "        top_docs = [hit[\"_source\"][\"body\"] for hit in response[\"hits\"][\"hits\"][:top_n]]\n",
    "        return \"\\n\".join(top_docs)\n",
    "\n",
    "    #combine user's query, conversation history, and docs from Elasticsearch to send to LLM\n",
    "    def augment (self, query, temperature=0.00001):\n",
    "        client = OpenAI(api_key=openai_api_key)\n",
    "        self.history.append({\"role\": \"user\", \"content\": query})\n",
    "        retrieval = Elasticsearch_rag()\n",
    "        retrieved = retrieval.retrieve(query)\n",
    "        prompt = ( \"This is the query: \"  +  query +  \" Here are supporting documents. \" + retrieved)\n",
    "        self.history.append({\"role\": \"user\", \"content\": query})\n",
    "        response = client.chat.completions.create(\n",
    "            messages=self.history,\n",
    "            model=self.model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        response_llm = str(response.choices[0].message.content)\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response_llm})\n",
    "        return response_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6e135-61e9-44d0-ab81-8c244aa3163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = Elasticsearch_rag()   # an instance of a conversation\n",
    "print(conversation.augment(\"What is Kibana good for?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43945a-ef31-481a-be9c-c54bf3f60692",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.augment(\"Can I run Kibana in a Docker container?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f66b9-c1be-42f1-9b53-739525ae1a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.augment(\"What was the first question I asked?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0734d268-a824-4c6f-9800-9f88a39e3e77",
   "metadata": {},
   "source": [
    "Congratulations!  We have examined how to create a RAG application that feeds documents from Elasticsearch to OpenAI's GPT LLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
