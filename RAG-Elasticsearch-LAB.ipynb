{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e1e98bb-b116-495f-9221-b38f3159a6b7",
   "metadata": {},
   "source": [
    "# Interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583615eb-5d10-413f-80f2-a65cf808c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6cc243-4c2b-4317-bc2b-8daea3ebd29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0574df5e-cfca-452f-9621-ef0711855ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env.instruqt')\n",
    "openai_api_key =  os.environ.get(\"LLM_APIKEY\") \n",
    "url = os.environ.get(\"LLM_PROXY_URL\") \n",
    "openai_api_base = f\"https://{url}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b42c2-44ff-4f06-8ec8-f49591f71f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"OPENAI_BASE_URL\"] = openai_api_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa78bd-06f7-45e5-b5cc-449273e5a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1a5e9-bbe3-4a2d-b2bf-a2524c3c8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_host = os.getenv(\"ELASTICSEARCH_URL\", None)\n",
    "es_api_key = os.getenv(\"ELASTICSEARCH_APIKEY\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec541826-4654-400f-9647-b7e2cdd6c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "     hosts=[f\"{es_host}\"],\n",
    "     api_key=es_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d5fd3-e9c1-44dd-a4fe-b9ad55f0b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM is from OpenAI \n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396d44b-1eea-48ad-b9b6-580becf231f1",
   "metadata": {},
   "source": [
    "fill in the missing code in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7058166-9f5f-4e1b-b7e8-41c4966724d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatWithLlm:\n",
    "    def __init__(self,systems_prompt=\"assistant\",model=\"gpt-4.1\"):\n",
    "        self.systems_prompt = systems_prompt\n",
    "        self.model = model\n",
    "        self.history = [{\"role\":\"system\",  \n",
    "                         \"content\":systems_prompt}]          #history helps us \"keep memory\" of what happened before\n",
    "   \n",
    "     def augment (self, query, temperature=0.00001):\n",
    "        client = OpenAI(< missing >)\n",
    "        self.< missing  code for memory >\n",
    "        retrieval = Elasticsearch_rag()\n",
    "        retrieved = retrieval.retrieve(query)\n",
    "        prompt = ( \"This is the query: \"  +  query +  \" Here are supporting documents. \" + retrieved)\n",
    "        self.< missing  code for memory >({\"role\": \"user\", \"content\": query})\n",
    "        response = client.chat.completions.create(\n",
    "            messages=self.history,\n",
    "            model=self.model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        response_llm = str(response.choices[0].message.content)\n",
    "        self.< missing  code for memory >\n",
    "        return response_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da52cd-3d47-413c-ae6a-88c3f18df18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with an instance of the ChatWithLlm class\n",
    "chat = ChatWithLlm(\"You're a helpful assistant\")\n",
    "llm_answer =  chat.augment(\"What is 2 + 2?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560a471-651b-4aa1-90b0-46422cfe90d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer = chat.augment(\"What did I just ask you?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8faaca5-5d4a-40ba-8686-b8a748cfd912",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer =  chat.augment(\"How did you remember what was asked?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a5784-8052-43cf-b8a9-cb822aaa461b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce1413-2688-49d7-ae8b-98fa75169d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b41a7b-a452-4044-bf42-99196f1bbb09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab113894-2fb9-4280-9545-382fc0650e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b2b19d8-50f6-4d28-854c-c2d9ca0f0e3a",
   "metadata": {},
   "source": [
    "answer below (scroll down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88a09e-5381-4e82-830d-32e31812105e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167aaf28-56ad-4dfb-b137-76af67c946ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93cfd6c-d2c2-4d45-9328-3e7a709699dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b2911-7890-45d1-b189-7620ee35f9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512c82a-4473-41d9-8f91-6a464542094d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0859eb-1f38-4e0a-a082-466330741d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ae006-bd01-4371-9e2f-ad89a90ac9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689368d0-a865-4ffc-89de-15c37361a009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26762b12-e13a-4aec-a91f-05e1d372fa95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a9286-c132-4e21-b6bc-c98d03970268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97d721-4d35-4536-84ce-8cd9fd96bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatWithLlm:\n",
    "    def __init__(self,systems_prompt=\"assistant\",model=\"gpt-4.1\"):\n",
    "        self.systems_prompt = systems_prompt\n",
    "        self.model = model\n",
    "        self.history = [{\"role\":\"system\",  \n",
    "                         \"content\":systems_prompt}]          #history helps us \"keep memory\" of what happened before\n",
    "   \n",
    "    def augment(self, users_prompt, temperature=0.00001):   #low temperature means consistent LLM responses (high means more creative)\n",
    "        client = OpenAI(api_key=openai_api_key)\n",
    "        self.history.append({\"role\": \"user\", \"content\": users_prompt})   #user role prompts the LLM \n",
    "        response = client.chat.completions.create(\n",
    "            messages=self.history,\n",
    "            model=self.model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        response_llm = str(response.choices[0].message.content)\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response_llm})\n",
    "        return response_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f5910-fb8c-48d0-b0ad-ed0ed456ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with an instance of the ChatWithLlm class\n",
    "chat = ChatWithLlm(\"You're a helpful assistant\")\n",
    "llm_answer =  chat.augment(\"What is 2 + 2?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41966b48-e016-44af-ac48-04a137d9dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer = chat.augment(\"What did I just ask you?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93b8bb-ca4a-4473-b060-2ce3aa5157a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer =  chat.augment(\"How did you remember what was asked?\")\n",
    "print(llm_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
