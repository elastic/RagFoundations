{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71892a06-c8b6-4e12-a5de-8a6ff6b30b40",
   "metadata": {},
   "source": [
    "# Interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583615eb-5d10-413f-80f2-a65cf808c004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.11/site-packages (1.101.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: dotenv in /opt/anaconda3/lib/python3.11/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.11/site-packages (from dotenv) (0.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a6cc243-4c2b-4317-bc2b-8daea3ebd29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0574df5e-cfca-452f-9621-ef0711855ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env.instruqt')\n",
    "openai_api_key =  os.environ.get(\"LLM_APIKEY\") \n",
    "url = os.environ.get(\"LLM_PROXY_URL\") \n",
    "openai_api_base = f\"https://{url}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7b42c2-44ff-4f06-8ec8-f49591f71f12",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str expected, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOPENAI_API_KEY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m = openai_api_key\n\u001b[32m      2\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mOPENAI_BASE_URL\u001b[39m\u001b[33m\"\u001b[39m] = openai_api_base\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:684\u001b[39m, in \u001b[36m__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:758\u001b[39m, in \u001b[36mencode\u001b[39m\u001b[34m(value)\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: str expected, not NoneType"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"OPENAI_BASE_URL\"] = openai_api_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa78bd-06f7-45e5-b5cc-449273e5a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1a5e9-bbe3-4a2d-b2bf-a2524c3c8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_host = os.getenv(\"ELASTICSEARCH_URL\", None)\n",
    "es_api_key = os.getenv(\"ELASTICSEARCH_APIKEY\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec541826-4654-400f-9647-b7e2cdd6c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "     hosts=[f\"{es_host}\"],\n",
    "     api_key=es_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d5fd3-e9c1-44dd-a4fe-b9ad55f0b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM is from OpenAI \n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b053e1c-1c7c-42ce-91c4-b730638c8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment (self, query, temperature=0.00001):\n",
    "        client = OpenAI(api_key=openai_api_key)\n",
    "        self.history.append({\"role\": \"user\", \"content\": query})\n",
    "        retrieval = Elasticsearch_rag()\n",
    "        retrieved = retrieval.retrieve(query)\n",
    "        prompt = ( \"This is the query: \"  +  query +  \" Here are supporting documents. \" + retrieved)\n",
    "        self.history.append({\"role\": \"user\", \"content\": query})\n",
    "        response = client.chat.completions.create(\n",
    "            messages=self.history,\n",
    "            model=self.model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        response_llm = str(response.choices[0].message.content)\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response_llm})\n",
    "        return response_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb6e6c6-ea6c-4548-a846-c388f6a07c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "llms_answer = augment (\"You're a helpful assistant\", \"What is 2+2?\")\n",
    "print(llms_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aa7ee8-3fa6-4123-a4ba-8c8b6d14d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start with a simple, one-pass interacation with the LLM. The function call2llm takes a systems_prompt, which is the \n",
    "#persona the system assumes in the interaction, and \"users_prompt\" which is the input from the user chatting with the LLM\n",
    "\n",
    "def call2llm(systems_prompt, users_prompt):\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": systems_prompt},\n",
    "            {\"role\": \"user\", \"content\": users_prompt}\n",
    "        ],\n",
    "        model=\"gpt-4.1\",\n",
    "        temperature=0.000001  # low means consistent LLM responses (high means more creative)\n",
    "    )\n",
    "    response = response.choices[0].message\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c11ff-ea9f-415d-ab26-bf92b431ce5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test\n",
    "llm_answer = call2llm(\"You're a helpful assistant\", \"What is 2+2?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b3aed-1ba0-4281-8e0e-9b7404d3a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer2 = call2llm(\"You're a helpful assistant\", \"What did we just sum?\")\n",
    "print(llm_answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396d44b-1eea-48ad-b9b6-580becf231f1",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97d721-4d35-4536-84ce-8cd9fd96bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatWithLlm:\n",
    "    def __init__(self,systems_prompt=\"assistant\",model=\"gpt-4.1\"):\n",
    "        self.systems_prompt = systems_prompt\n",
    "        self.model = model\n",
    "        self.history = [{\"role\":\"system\",  \n",
    "                         \"content\":systems_prompt}]          #history helps us \"keep memory\" of what happened before\n",
    "   \n",
    "    def call2llm(self, users_prompt, temperature=0.00001):   #low temperature means consistent LLM responses (high means more creative)\n",
    "        client = OpenAI(api_key=openai_api_key)\n",
    "        self.history.append({\"role\": \"user\", \"content\": users_prompt})   #user role prompts the LLM \n",
    "        response = client.chat.completions.create(\n",
    "            messages=self.history,\n",
    "            model=self.model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        response_llm = str(response.choices[0].message.content)\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response_llm})\n",
    "        return response_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f5910-fb8c-48d0-b0ad-ed0ed456ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with an instance of the ChatWithLlm class\n",
    "chat = ChatWithLlm(\"You're a helpful assistant\")\n",
    "llm_answer =  chat.call2llm(\"What is 2 + 2?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41966b48-e016-44af-ac48-04a137d9dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer =  chat.call2llm(\"What did I just ask you?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93b8bb-ca4a-4473-b060-2ce3aa5157a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer =  chat.call2llm(\"How did you remember what was asked?\")\n",
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3b313-b2eb-4cf9-92f9-9316e6a07512",
   "metadata": {},
   "source": [
    " <br>\n",
    " <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
